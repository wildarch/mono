\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{{images/}}

% Packages that are sometimes useful
%\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
%\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Reviving the Lost Art of Combinator Graph Reduction},
    pdfpagemode=FullScreen,
    }

\urlstyle{same}

\usepackage{amsmath}
\DeclareMathOperator{\strict}{strict}
\DeclareMathOperator{\lazy}{lazy}

\begin{document}

\title{Reviving the Lost Art of Combinator Graph Reduction}

\author{\IEEEauthorblockN{Daan de Graaf}
    \IEEEauthorblockA{\textit{Master's student Computer Engineering} \\
        \textit{TU Delft / Eindhoven University of Technology}\\
        Eindhoven, Netherlands \\
        d.j.a.degraaf@student.tudelft.nl / d.j.a.d.graaf@student.tue.nl}
}

\maketitle

\begin{abstract}
    % Rules of thumb
    % - 1 sentence background
    % - 1 sentence motive/problem
    % - 1 sentence objective
    % - 1 sentence approach/method
    % - 2 sentences support
    % - 1 sentence conclusion
    % - 1 sentence implication
    % - 1 sentence limitation
    This is the abstract.
\end{abstract}

\begin{IEEEkeywords}
    combinatory logic, graph reduction, lazy functional language, lazy evaluation
\end{IEEEkeywords}

\section{Introduction}
% Why did I research this? Give the motive, usually a knowledge gap or problem
Lazy evaluation of programs is a powerful idea.
It allows a program to operate on infinitely large data structures, and if part of the computation turns out to be unnecessary, it is more efficient than a strict evaluation.
In practice, delaying computation often introduces overhead, making languages based on lazy evaluation slower than their strict counterparts.
To compete with a strict implementation, a lazy runtime must maximize laziness while minimizing overhead.
It is a difficult balance, and over the years many different evaluator designs, or \textit{abstract machines}, have been proposed~\cite{turner_new_1979,kieburtz_g-machine_1985,fairbairn_tim_1987,burn_spineless_1988,koopman_fresh_1989,jones_implementing_nodate}.

One of the early approaches to implementing lazy functional languages is combinator graph reduction~\cite{turner_new_1979}.
It was the basis for David Turner's Miranda language~\cite{turner_miranda_1985}, the popular choice for pure functional languages and one of the few to be commercially supported at the time.
Eventually, the interest in Miranda and combinator graph reduction with it waned.
Part of the reason may be that Miranda was until very recently a closed-source, licensed software (its source code was released only in 2020~\cite{noauthor_open_2021}), leading to the creation of the open-source Haskell language~\cite{hudak_history_2007}.
But combinator reduction also deserves part of the blame.
% Firstly, owing to the simplicity of individual combinators, expressions are reduced in many small steps, so the per-step overhead greatly impacts runtime performance.
% Every reduction step modifies the stack and usually also the graph, resulting in high memory traffic.
One of the major issues is that the translation to combinators may result in much larger programs.
For a lambda calculus expression of size $N$, the traditional bracket abstraction algorithm produces an output of $O(N^2)$ in the worst case, where other approaches such as supercombinators are linear\cite{spj_impl}.
Larger programs run slower, and along with other disadvantages, this led to combinator graph reduction being all but abandoned.

% What did I research?
% - Research problem/question
%   * Definitions
%   * Choices
%   * Presuppositions
% - Why specifically this research question?
%   * Theoretical or societal relevance
Recently, however, a new algorithm for the translation of lambda calculus expressions into combinatory logic has been proposed that is linear in the size of the input\cite{kiselyov_lambda_2018}.
Kiselyov's translation algorithm is based on semantics rather than syntax and this deeper understanding of the program has been shown to produce more compact translations.
While a clear improvement over previous translation strategies, it remains to be seen how big the impact is on real-world performance.
Is it enough to bridge the performance gap with newer reduction engines based on different paradigms?
This leads us to our first research question:

\textbf{RQ1:} Does Kiselyov's semantic translation make combinator graph reduction competitive with contemporary lazy functional evaluators?

% How did I research this?
% - Methods
% - Sub-questions
We use GHC as the reference for a contemporary evaluator, and then measure the execution time of a set of benchmark programs.
We also need a fast reduction engine that can run programs compiled to combinator expressions.
Alas, there is no performant reduction engine tailored to executing such programs.
The now open-sourced Miranda is a good starting point, but it is showing its age.
The 2020 port compiles cleanly on a recent GCC, but enabling optimizations is known to break the garbage collector.
Miranda's architecture is based on cells with a tag, head and tail as described in\cite{turner_new_1979}, but later work has shown that a tagless representation with direct pointers to code leads to better performance\cite{koopman_fresh_1989}.
In absence of an up-to-date reference on building a practical combinator reduction engine, we formulate the second research question:

\textbf{RQ2:} How is combinator graph reduction implemented efficiently on conventional hardware?

We evaluate the state of the art in combinator graph reduction and build a fast implementation for a language that is simple but powerful enough to express our benchmark programs.
Using this engine, we can then answer \textbf{RQ1}.

% How did I organize the report?
% - Structure Overview
Concretely, our contributions are the following:
\begin{itemize}
    \item Kiselyov develops not one but two algorithms. The first does not have linear complexity, but tends to produce very compact code.
          The second is guaranteed to be of linear complexity, at the cost of being less compact in certain cases.
          Kiselyov's description is somewhat theoretical, so in section~\ref{sec:kiselyov} we describe both algorithms in a way that is straightforward to implement.
    \item We implement a fast combinator graph reduction engine based on the ideas in Miranda~\cite{turner_miranda_1985} and later improvements~\cite{koopman_architecture_1992}.
          We elaborate on its design in section~\ref{sec:engine}.
    \item We evaluate the performance of Miranda, Haskell and our engine on a set of benchmark programs in section~\ref{sec:eval}.
\end{itemize}

For readers unfamiliar with combinatory logic and/or graph reduction, we provide a short introduction to these topics in section~\ref{sec:prelim}.

\section{Preliminaries}
\label{sec:prelim}
\textbf{TODO: write section. Currently contains only a few small fragments}
\subsection{Combinatory Logic}
Starting from a lambda calculus expression, we first translate it into an expression consisting exclusively of the application of a small set of predefined functions, or \textit{combinators}.
It turns out that any lambda calculus expression can be expressed using just three combinators:
\begin{itemize}
    \item $S \ f \ g \ x = f \ x \ (g \ x)$
    \item $K \ x \ y = x$
    \item $I \ x = x$\footnote{It turns out that $SKK = I$, so even this combinator is not strictly necessary, but all reasonable implementations do include $I$, and it simplifies the example.}
\end{itemize}

For example, $\lambda x.x+1$ can be converted into the combinator expression:

\[
    S \ (S \  (K \  +) \  I) \ (K \ 1)
\]

We invite the reader to verify this by applying $x$ to this expression and reducing it to $x+1$ with the usual $\beta$-reduction rule.
While the generated expression is not as easy to read, the simplicity of its components is appealing: to execute a program compiled to combinators, an evaluator only needs to support four operations: the built-in $S$, $K$ and $I$ combinators, and function application\footnote{And any builtins of the language, like the plus operator in the example.}.
Better still, conversion to combinators removes all bound variables.
The runtime need not concern itself with passing arguments, this is all handled by the combinators.
The compiled program is executed using graph reduction, a technique that is also common in other reduction engines\cite{kieburtz_g-machine_1985,fairbairn_tim_1987}.

\subsection{Graph Reduction}
% In the reduction graph, vertices are combinators, constants or function applications, and edges connect the function and argument for application (combinators and constants have no outgoing edges, they are leaf nodes).


\section{Kiselyov's algorithms}
\label{sec:kiselyov}
Kiselyov's paper~\cite{kiselyov_lambda_2018} describes three iterations of his non-linear semantic translation, and finally the linear algorithm.
They are presented as a type system accompanied with operational semantics, with the exception of the $\eta$-optimization given as an OCaml code patch.
OCaml's type system can represent these algorithms very concisely in tagless-final style, but how should we represent them in a more conventional style, possibly in a language that does not have such a strong type system?

\subsection{Strict algorithm}
This is based on the rules in figure 6 of Kiselyov's paper.
We call it \textit{strict} it does not have lazy weakening: most notably it lacks the K-optimization required for full laziness.

The most naive implementation is to perform the type checking, then use the built context to apply the elimination rules and the translated program.
As Kiselyov already notes in his treatment of the linear algorithm further on in the paper, the translation does not actually care what elements are in the context, just that we keep track of the size of the context.
This lets us express compilation as a recursive function:

\begin{equation*}
    \begin{array}{l l l l}
        \strict & z         & = & 1 \models I                                              \\
        \strict & s \ e     & = & n+1 \models (0 \models K) \coprod (n \models c)          \\
                &           &   & \text{where } (n \models c) := \strict e                 \\
        \\
        \strict & \lambda e & = & 0 \models K \; c                                         \\
                &           &   & \text{where } (0 \models c) := \strict e                 \\
        \strict & \lambda e & = & (n-1) \models K \; c                                     \\
                &           &   & \text{where } (n \models c) := \strict e                 \\
        \\
        \strict & e_1 \ e_2 & = & (n \models ((n_1 \models c_1) \coprod (n_2 \models c_2)) \\
                &           &   & \text{where } (n_1 \models c_1) := \strict e_1           \\
                &           &   & \text{where } (n_2 \models c_2) := \strict e_2           \\
                &           &   & \text{where } n := \max n_1 \, n_2                       \\
    \end{array}
\end{equation*}

There are two cases for $\lambda e$, and this is precisely where the number of elements in the context matters:
if the context is empty, the first rule applies, otherwise, we use the second one.
The semantic function may be defined as:

\begin{equation*}
    \arraycolsep=1.4pt
    \begin{array}{l l l l l}
        (0 \models c_1)   & \coprod & (0 \models c_2)   & = & c_1 \ c_2                                         \\
        \\
        (0 \models c_1)   & \coprod & (n_2 \models c_2) & = & (0 \models B c_1) \coprod (n_2 - 1 \models c_2)   \\
        \\
        (n_1 \models c_1) & \coprod & (0 \models c_2)   & = & (0 \models C C c_2) \coprod (n_1 - 1 \models c_1) \\
        \\
        (n_1 \models c_1) & \coprod & (n_2 \models c_2) & = & (n_1 - 1 \models (s \coprod l)) \coprod r)        \\
                          &         &                   &   & \text{where } s := (0 \models S )                 \\
                          &         &                   &   & \text{where } l := (n_1-1 \models c_1 )           \\
                          &         &                   &   & \text{where } r := (n_2-1 \models c_2 )           \\
    \end{array}
\end{equation*}

These simplified functions can be implemented directly and efficiently in a conventional programming language.

\subsection{Lazy weakening}
Adding lazy weakening requires that we track which items in the context are ignored.
We change our context representation from an integer to a list of boolean values.
In the new compilation functions below, we use a shorthand $t$ for the value \textit{true} and $f$ for \textit{false}.
We always match the last element of lists and represent the remainder of the list with $\Gamma$.

\begin{equation*}
    \begin{array}{l l l l}
        \lazy & z         & = & t \models I                                                             \\
        \lazy & s \ e     & = & \Gamma,f \models c                                                      \\
              &           &   & \text{where } (\Gamma \models c) := \lazy e                             \\
        \\
        \lazy & \lambda e & = & \emptyset \models K \; c                                                \\
              &           &   & \text{where } (\emptyset \models c) := \lazy e                          \\
        \lazy & \lambda e & = & (\emptyset \models K) \coprod (\Gamma \models c)                        \\
              &           &   & \text{where } (\Gamma,f \models c) := \lazy e                           \\
        \lazy & \lambda e & = & \Gamma \models K \; c                                                   \\
              &           &   & \text{where } (\Gamma,t \models c) := \lazy e                           \\
        \\
        \lazy & e_1 \ e_2 & = & (\Gamma \models ((\Gamma_1 \models c_1) \coprod (\Gamma_2 \models c_2)) \\
              &           &   & \text{where } (\Gamma_1 \models c_1) := \lazy e_1                       \\
              &           &   & \text{where } (\Gamma_2 \models c_2) := \lazy e_2                       \\
              &           &   & \text{where } \Gamma := \Gamma_1 \sqcup \Gamma_2                        \\
    \end{array}
\end{equation*}

We have introduced yet another case for $\lambda e$, this time to distinguish between $t$ or $f$ as the last element in the context of the inner expression.
There is also the new function $\sqcup$, which we need to implement:

\begin{equation*}
    \begin{array}{l l l l l}
        \Gamma_1    & \sqcup & \emptyset   & = & \Gamma_1                     \\
        \emptyset   & \sqcup & \Gamma_2    & = & \Gamma_2                     \\
        \Gamma_1,t  & \sqcup & \Gamma_2,\_ & = & (\Gamma_1 \sqcup \Gamma_2),t \\
        \Gamma_1,\_ & \sqcup & \Gamma_2,t  & = & (\Gamma_1 \sqcup \Gamma_2),t \\
        \Gamma_1,f  & \sqcup & \Gamma_2,f  & = & (\Gamma_1 \sqcup \Gamma_2),f \\
    \end{array}
\end{equation*}

In our implementation, we unroll the recursion into a while loop, and repeatedly pop elements from the two arrays until both are empty.
This builds the new context backwards, so as a final step the array is reversed.

Alternatively, an implementation can pad the shorter list (at the start!) with $f$ and zip them.
This is particularly convenient for languages that provide a zip function with a default value\footnote{such as \href{https://docs.python.org/3/library/itertools.html\#itertools.zip\_longest}{\texttt{ziplongest}} in Python.}.

Our semantic function now has to handle more cases, and thus is somewhat larger:

\begin{equation*}
    \arraycolsep=1.4pt
    \begin{array}{l l l l l}
        (\emptyset \models c_1)  & \coprod & (\emptyset \models c_2)  & = & c_1 \ c_2                                                \\
        \\
        (\emptyset \models c_1)  & \coprod & (\Gamma_2,t \models c_2) & = & (\emptyset \models B c_1) \coprod (\Gamma_2 \models c_2) \\
        \\
        (\Gamma_1,t \models c_1) & \coprod & (\emptyset \models c_2)  & = & (\emptyset \models C C c_2) \coprod (\Gamma \models c_1) \\
        \\
        (\Gamma_1,t \models c_1) & \coprod & (\Gamma_2,t \models c_2) & = & (\Gamma_1 \models (s \coprod l)) \coprod r)              \\
                                 &         &                          &   & \text{where } s := (\emptyset \models S )                \\
                                 &         &                          &   & \text{where } l := (\Gamma_1 \models c_1 )               \\
                                 &         &                          &   & \text{where } r := (\Gamma_2 \models c_2 )               \\
        \\
        % The new additions
        (\Gamma_1,f \models c_1) & \coprod & (\Gamma_2,f \models c_2) & = & (\Gamma_1 \models c_1) \coprod (\Gamma_2 \models c_2)    \\
        \\
        (\Gamma_1,f \models c_1) & \coprod & (\Gamma_2,t \models c_2) & = & (\Gamma_1 \models (b \coprod l)) \coprod r)              \\
                                 &         &                          &   & \text{where } b := (\emptyset \models B )                \\
                                 &         &                          &   & \text{where } l := (\Gamma_1 \models c_1 )               \\
                                 &         &                          &   & \text{where } r := (\Gamma_2 \models c_2 )               \\
        \\
        (\Gamma_1,t \models c_1) & \coprod & (\Gamma_2,f \models c_2) & = & (\Gamma_1 \models (c \coprod l)) \coprod r)              \\
                                 &         &                          &   & \text{where } c := (\emptyset \models C )                \\
                                 &         &                          &   & \text{where } l := (\Gamma_1 \models c_1 )               \\
                                 &         &                          &   & \text{where } r := (\Gamma_2 \models c_2 )               \\
        \\
        (\emptyset \models c_1)  & \coprod & (\Gamma_2,f \models c_2) & = & (\emptyset \models c_1) \coprod (\Gamma_2 \models c_2)   \\
        \\
        (\Gamma_1,f \models c_1) & \coprod & (\emptyset \models c_2)  & = & (\Gamma_1 \models c_1) \coprod (\emptyset \models c_2)   \\
    \end{array}
\end{equation*}

The first 4 cases are direct translations from the previous definitions, the others are new additions to handle ignored elements.
The last 2 cases are not included in the original paper, but they are necessary to make the pattern matching exhaustive, and are present in the reference OCaml implementation.

\textbf{TODO:} Eta optimization.

\section{Engine implementation}
\label{sec:engine}
\subsection*{Miranda}
Describe the implementation of a SASL/Miranda-style engine.

Compare performance with Open Source Miranda.

\subsection*{TIGRE}
Describe changes made to align with TIGRE.

Compare performance with Miranda-style engine.

\section{Performance evaluation}
\label{sec:eval}
Programs:
\begin{itemize}
    \item ackermann
    \item digits of e
    \item linfib
    \item lsort
    \item nfib
    \item primes
    \item queens
    \item tak
    \item towers of hanoi
    \item treesort
\end{itemize}

Evaluate performance of:

\begin{itemize}
    \item Bracket abstraction
    \item Kiselyov
    \item GHC
\end{itemize}

\section{Conclusion}
% Repeat the main question / objective of your research

% Give the answer to the question
Source code for our Superg evaluator is available at \url{https://github.com/wildarch/mono/tree/main/experiments/superg}.

% Why is this the answer? Present the main arguments

% What are the implications of this answer?

% Suggestions for future research

% FINAL CHECK: Can the conclusion be read independently?

\bibliographystyle{IEEEtran}
\bibliography{zotero, references}

\end{document}
